{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Dir: /Users/victor_naidu/GitHub/amex-default-prediction\n"
     ]
    }
   ],
   "source": [
    "# Change working directory to project root\n",
    "if os.getcwd().split(\"/\")[-1] == \"notebooks\":\n",
    "    # Change dir to parent directory.\n",
    "    os.chdir(\"../\")\n",
    "    # Print the current working directory\n",
    "    print(f'Current Dir: {os.getcwd()}')\n",
    "    \n",
    "# Enable garbage collection\n",
    "gc.enable()\n",
    "\n",
    "# > Configure display options for Pandas\n",
    "# ---------------------------------------------------------------\n",
    "# Set display width to 1000.\n",
    "pd.set_option(\"display.width\", 1000)\n",
    "# Set maximum number of rows to display to 500.\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "# Set maximum number of columns to display to 500.\n",
    "pd.set_option(\"display.max_columns\", 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset, use_feather=True):\n",
    "    '''> Function takes in the name of a dataset, and then returns the feather or csv file for the dataset. [TODO: update content]\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset\n",
    "        The dataset to load.\n",
    "    use_feather, optional\n",
    "        A boolean that determines whether to use the feather file or the csv file.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        the DataFrame for the dataset.\n",
    "    \n",
    "    '''\n",
    "    # Function to load amex data\n",
    "    # Raw Data - Source: American Express via Kaggle\n",
    "    # Reduced  - Source: https://www.kaggle.com/datasets/munumbutt/amexfeather\n",
    "\n",
    "    # Tuple of strings that are valid dataset names that can be loaded.\n",
    "    valid_datasets = (\"train\", \"test\", \"train_agg\", \"test_agg\")\n",
    "\n",
    "    # Check `dataset` parameter type. If the type is not a string, then raise a TypeError. \n",
    "    # If dataset is not in the list of valid datasets, then raise a ValueError.\n",
    "    if not isinstance(dataset, str):\n",
    "        raise TypeError\n",
    "    elif dataset not in valid_datasets:\n",
    "        raise ValueError\n",
    "\n",
    "    # A dictionary of featureset dictionaries. \n",
    "    # The outer dictionary has two keys to select data format: \n",
    "    # ----> `feather`, `csv`\n",
    "    # The inner dictionaries have four keys corresponding to datasets: \n",
    "    # ----> `train`, `test`, `train_agg`, `test_agg`\n",
    "    # Values correspond to file paths for feather and csv files for that dataset.\n",
    "    data_filepaths = {\n",
    "        \"feather\": {\n",
    "            \"train\": \"./data/external/compressed/train_data.ftr\",\n",
    "            \"test\": \"./data/external/compressed/test_data.ftr\",\n",
    "            \"train_agg\": \"./data/interim/train_agg.ftr\",\n",
    "            \"test_agg\": \"./data/interim/test_agg.ftr\",\n",
    "        },\n",
    "        \"csv\": {\n",
    "            \"train\": \"./data/raw/train_data.csv\",\n",
    "            \"test\": \"./data/raw/test_data.csv\",\n",
    "            \"train_agg\": \"./data/interim/train_agg.csv\",\n",
    "            \"test_agg\": \"./data/interim/test_agg.csv\",\n",
    "        },\n",
    "    }\n",
    "    if use_feather:\n",
    "        # Set path to feather file for the dataset.\n",
    "        feather_file = data_filepaths[\"feather\"].get(dataset)\n",
    "        # Return feather file as Pandas DataFrame; set index to the customer ID.\n",
    "        return pd.read_feather(feather_file).set_index(\"customer_ID\")\n",
    "    else:\n",
    "        # Set path to *.csv file for the dataset.\n",
    "        csv_file = data_filepaths[\"csv\"].get(dataset)\n",
    "        # Return csv file as Pandas DataFrame using customer_ID as the index column\n",
    "        return pd.read_csv(csv_file, index_col=\"customer_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_incomplete_features(amex_dataset, threshold=0.85, verbose=True):\n",
    "    '''> Function takes in a dataset and a threshold value, and returns a set of features that have a percentage of missing values that is greater than or equal to the threshold. [TODO: update content]\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    amex_dataset\n",
    "        The dataset that we want to check for incomplete features.\n",
    "    threshold\n",
    "        The percentage of missing values that a feature must have to be considered incomplete.\n",
    "    verbose, optional\n",
    "        If True, prints the number of features in each category.\n",
    "    \n",
    "    '''\n",
    "    # Check the type and that the value of the `threshold`` parameter is in range: \n",
    "    if not isinstance(threshold, (float, int)):\n",
    "        # If invalid (not a float or an integer), than raise a TypeError. \n",
    "        raise TypeError()\n",
    "    elif (threshold < 0) or (threshold > 1):\n",
    "        # If invalid (not between 0 and 1), than raise a ValueError.\n",
    "        raise ValueError()\n",
    "\n",
    "    # Create a Series containing the percentage missing or null for each column in the dataset.\n",
    "    pct_incomplete = amex.isnull().sum().div(len(amex)).sort_values(ascending=False)\n",
    "\n",
    "    # Create set containing incomplete features, or column names where the percentage of missing or null values are greater than or equal to the assigned threshold.\n",
    "    incomplete_features = set(\n",
    "        pct_incomplete[pct_incomplete >= threshold].index.tolist()\n",
    "    )\n",
    "    # Print the number of incomplete features for a given threshold.\n",
    "    if verbose:\n",
    "        print(f\"Incomplete Features >= {threshold}%:\\n{incomplete_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_agg_features(amex_dataset):\n",
    "    '''Function to create new features by aggregating the dataset by [TODO: add content].\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    amex_dataset\n",
    "        The dataset that we want to create the aggregated features for.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        DataFrame of aggregated features derived from the original dataset.\n",
    "    \n",
    "    '''\n",
    "    # Select numeric features from the dataset\n",
    "    amex_numeric = amex_dataset.select_dtypes(include=\"number\")\n",
    "\n",
    "\n",
    "    # Create a new dataframe with the first and last rows of the dataframe.\n",
    "    last_statement = amex_numeric.groupby(\"customer_ID\").nth(-1)\n",
    "    first_statement = amex_numeric.groupby(\"customer_ID\").nth(0)\n",
    "\n",
    "    # Divide the last statement by the first statement and filling the NaN values with 1.\n",
    "    lag_div = (last_statement\n",
    "               .div(first_statement)\n",
    "               .replace(-np.inf, 0)\n",
    "               .replace(np.inf, 2)\n",
    "               .fillna(1))\n",
    "\n",
    "    # Subtract the first statement from the last statement and filling the NaN values with 0.\n",
    "    lag_diff = (last_statement\n",
    "                .subtract(first_statement)\n",
    "                .replace([np.inf, -np.inf], np.nan)\n",
    "                .fillna(0))\n",
    "\n",
    "    # Rename column names by appending lad_type previous name.\n",
    "    lag_div.columns = [col + \"__lag_div\" for col in lag_div.columns]\n",
    "    lag_diff.columns = [col + \"__lag_diff\" for col in lag_diff.columns]\n",
    "\n",
    "    # Concate `lag_diff` and `lag_div` to create a new DataFrame with aggregated lag features.\n",
    "    numeric__agg_lag = pd.concat([lag_diff, lag_div], axis=1)\n",
    "\n",
    "    # Group columns by customer_ID and calculate aggregated features with statistical measures\n",
    "    numeric__agg = amex_numeric.groupby(\"customer_ID\").agg(\n",
    "        [\"last\", \"mean\", \"min\", \"max\", \"std\"]\n",
    "    )\n",
    "    # Rename each column by joining the two levels of column names.\n",
    "    numeric__agg.columns = [\"__\".join(col) for col in numeric__agg.columns]\n",
    "\n",
    "    # Group data by the customer ID; \n",
    "    # Select the columns with categorical data; \n",
    "    # Lastly, apply aggregate functions.\n",
    "    categorical__agg = (\n",
    "        amex_dataset.select_dtypes(include=\"category\")\n",
    "        .groupby(\"customer_ID\")\n",
    "        .agg([\"first\", \"last\", \"count\", \"nunique\"])\n",
    "    )\n",
    "\n",
    "    # Rename each column by joining the two levels of column names.\n",
    "    categorical__agg.columns = [\"__\".join(col) for col in categorical__agg.columns]\n",
    "\n",
    "    # Concatenating the `categorical__agg`, `numeric__agg`, and `numeric__agg_lag` dataframes.\n",
    "    amex_agg = pd.concat([categorical__agg, numeric__agg, numeric__agg_lag], axis=1)\n",
    "\n",
    "    # Returning the aggregated features for the dataset.\n",
    "    return amex_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(amex_dataset):\n",
    "    '''> Function takes in the amex dataset as a DataFrame; drops columns that are invalid, creates aggregated features for the dataset, and returns a DataFrame with the aggregated features.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    amex_dataset\n",
    "        The dataset that we want to create the aggregated features for.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        The aggregated features for the dataset.\n",
    "    \n",
    "    '''\n",
    "    # Create a set consisting of incomplete features.\n",
    "    incomplete_features = {\n",
    "        'D_87', 'D_88', 'D_108', 'D_110', 'D_111', 'B_39', \n",
    "        'D_73', 'B_42', 'D_134', 'D_137', 'D_135', 'D_138', \n",
    "        'D_136', 'R_9', 'B_29', 'D_106', 'D_132', 'D_49', \n",
    "        'R_26', 'D_76', 'D_66', 'D_42'}\n",
    "\n",
    "    # Create a set of features that are now redundant.\n",
    "    made_redundant = {'S_2'}\n",
    "\n",
    "    # Create a set for the target variable.\n",
    "    target_variable = {'target'}\n",
    "\n",
    "    # Create a tuple of invalid features by taking the union of the three sets\n",
    "    invalid_cols = (incomplete_features | made_redundant | target_variable)\n",
    "\n",
    "    # Create a list of columns that are in both the `amex_dataset` and `invalid_cols`\n",
    "    cols_to_drop = (amex_dataset\n",
    "                    .columns\n",
    "                    .intersection(invalid_cols)\n",
    "                    .tolist())\n",
    "\n",
    "    # 1) Drop columns that in `cols_to_drop` from the dataset\n",
    "    # 2) Create aggregated features for the dataset.\n",
    "    amex_aggregated = (amex_dataset\n",
    "                       .drop(cols_to_drop, axis=1)\n",
    "                       .pipe(create_agg_features))\n",
    "        \n",
    "    if 'target' in cols_to_drop:\n",
    "        # Add target variable to the DataFrame\n",
    "        amex_aggregated['target'] = (\n",
    "            amex_dataset.groupby('customer_ID').tail(1).target)\n",
    "    # Return DataFrame w/ aggregated features\n",
    "    return amex_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_datasets(read_feather=True, to_feather=True, to_csv=False):\n",
    "    '''Function loads the train and test datasets, makes features, and saves them to feather files\n",
    "    [TODO: add content]\n",
    "    Parameters\n",
    "    ----------\n",
    "    read_feather, optional\n",
    "        whether to read the feather files or not\n",
    "    to_feather, optional\n",
    "        whether to save the dataframe to feather format\n",
    "    to_csv, optional\n",
    "        If True, will save the dataframes to csv files.\n",
    "    \n",
    "    '''\n",
    "    # Load train and test datasets in feather format; pipe to make features for modeling.\n",
    "    train_agg = (load_dataset('train', use_feather=read_feather)\n",
    "                 .pipe(make_features))\n",
    "    test_agg = (load_dataset(\"test\", use_feather=read_feather)\n",
    "                .pipe(make_features))\n",
    "    \n",
    "    if to_feather:\n",
    "        # Save train_agg and test_agg datasets to disk as feather files.\n",
    "        train_agg.reset_index().to_feather('./data/interim/train_agg.ftr')\n",
    "        test_agg.reset_index().to_feather('./data/interim/test_agg.ftr')\n",
    "        \n",
    "    if to_csv:\n",
    "        # Save train_agg and test_agg datasets to disk as csv files.\n",
    "        train_agg.to_csv('./data/interim/train_agg.csv')\n",
    "        test_agg.to_csv('./data/interim/test_agg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_dict(X_train, verbose=True):\n",
    "    '''Function takes a dataset as a DataFrame and returns a dictionary of feature dictionaries.\n",
    "    [TODO: update content]\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train\n",
    "        The training dataset.\n",
    "    verbose, optional\n",
    "        If True, it will print the length of the numeric_features, categorical_features, and\n",
    "    ordinal_features.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        A dictionary with the keys and values of the numeric_features, categorical_features,\n",
    "    ordinal_features, and all_features.\n",
    "    \n",
    "    '''\n",
    "    # Select all numeric data within the dataset; access their column names; cast as list.\n",
    "    numeric_features = (X_train\n",
    "                        .select_dtypes(include='number')\n",
    "                        .columns\n",
    "                        .tolist())\n",
    "\n",
    "    # Select all categorical data within the dataset; access their column names; cast as list.\n",
    "    categorical_features = (X_train\n",
    "                            .select_dtypes(include='category')\n",
    "                            .columns\n",
    "                            .tolist())\n",
    "\n",
    "    # Create an empty list of ordinal features (for later use).\n",
    "    ordinal_features = []\n",
    "    \n",
    "    # Create list of all features by assigning the column names in the dataset to a list.\n",
    "    all_features = X_train.columns.tolist()\n",
    "    \n",
    "    if verbose:\n",
    "        #Print the number of features with numeric, categorical, or ordinal data.\n",
    "        print(f'Numeric Features - Count(#): {len(numeric_features)}')\n",
    "        print(f'Categorical Features - Count(#): {len(categorical_features)}')\n",
    "        print(f'Ordinal Features - Count(#): {len(ordinal_features)}')\n",
    "\n",
    "    # Return the dictionary of feature dictionaries\n",
    "    return {\n",
    "        'num': numeric_features,\n",
    "        'cat': categorical_features,\n",
    "        'ord': ordinal_features,\n",
    "        'all': all_features,\n",
    "        'numeric': numeric_features,\n",
    "        'categorical': categorical_features,\n",
    "        'ordinal': ordinal_features,\n",
    "        'all_features': all_features\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "28ed4e3bfc6dbc0ee6f9216b9beca2c6fff623d080b9c5b8a874eb933f3db8ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
