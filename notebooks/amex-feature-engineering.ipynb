{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Dir: /home/victor/amex-default-prediction\n"
     ]
    }
   ],
   "source": [
    "# Change working directory to project root\n",
    "if os.getcwd().split(\"/\")[-1] == \"notebooks\":\n",
    "    # Change dir to parent directory.\n",
    "    os.chdir(\"../\")\n",
    "    # Print the current working directory\n",
    "    print(f'Current Dir: {os.getcwd()}')\n",
    "    \n",
    "# Enable garbage collection\n",
    "gc.enable()\n",
    "\n",
    "# > Configure display options for Pandas\n",
    "# ---------------------------------------------------------------\n",
    "# Set display width to 1000.\n",
    "pd.set_option(\"display.width\", 1000)\n",
    "# Set maximum number of rows to display to 500.\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "# Set maximum number of columns to display to 500.\n",
    "pd.set_option(\"display.max_columns\", 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset, use_feather=True):\n",
    "    '''> Function takes in the name of a dataset, and then returns the feather or csv file for the dataset. [TODO: update content]\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset\n",
    "        The dataset to load.\n",
    "    use_feather, optional\n",
    "        A boolean that determines whether to use the feather file or the csv file.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        the DataFrame for the dataset.\n",
    "    \n",
    "    '''\n",
    "    # Function to load amex data\n",
    "    # Raw Data - Source: American Express via Kaggle\n",
    "    # Reduced  - Source: https://www.kaggle.com/datasets/munumbutt/amexfeather\n",
    "\n",
    "    # Tuple of strings that are valid dataset names that can be loaded.\n",
    "    valid_datasets = (\"train\", \"test\", \"train_agg\", \"test_agg\")\n",
    "\n",
    "    # Check `dataset` parameter type. If the type is not a string, then raise a TypeError. \n",
    "    # If dataset is not in the list of valid datasets, then raise a ValueError.\n",
    "    if not isinstance(dataset, str):\n",
    "        raise TypeError\n",
    "    elif dataset not in valid_datasets:\n",
    "        raise ValueError\n",
    "\n",
    "    # A dictionary of featureset dictionaries. \n",
    "    # The outer dictionary has two keys to select data format: \n",
    "    # ----> `feather`, `csv`\n",
    "    # The inner dictionaries have four keys corresponding to datasets: \n",
    "    # ----> `train`, `test`, `train_agg`, `test_agg`\n",
    "    # Values correspond to file paths for feather and csv files for that dataset.\n",
    "    data_filepaths = {\n",
    "        \"feather\": {\n",
    "            \"train\": \"./data/external/compressed/train_data.ftr\",\n",
    "            \"test\": \"./data/external/compressed/test_data.ftr\",\n",
    "            \"train_agg\": \"./data/interim/train_agg.ftr\",\n",
    "            \"test_agg\": \"./data/interim/test_agg.ftr\",\n",
    "        },\n",
    "        \"csv\": {\n",
    "            \"train\": \"./data/raw/train_data.csv\",\n",
    "            \"test\": \"./data/raw/test_data.csv\",\n",
    "            \"train_agg\": \"./data/interim/train_agg.csv\",\n",
    "            \"test_agg\": \"./data/interim/test_agg.csv\",\n",
    "        },\n",
    "    }\n",
    "    if use_feather:\n",
    "        # Set path to feather file for the dataset.\n",
    "        feather_file = data_filepaths[\"feather\"].get(dataset)\n",
    "        # Return feather file as Pandas DataFrame; set index to the customer ID.\n",
    "        return pd.read_feather(feather_file).set_index(\"customer_ID\")\n",
    "    else:\n",
    "        # Set path to *.csv file for the dataset.\n",
    "        csv_file = data_filepaths[\"csv\"].get(dataset)\n",
    "        # Return csv file as Pandas DataFrame using customer_ID as the index column\n",
    "        return pd.read_csv(csv_file, index_col=\"customer_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_incomplete_features(amex_dataset, threshold=0.85, verbose=True):\n",
    "    '''> Function takes in a dataset and a threshold value, and returns a set of features that have a percentage of missing values that is greater than or equal to the threshold. [TODO: update content]\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    amex_dataset\n",
    "        The dataset that we want to check for incomplete features.\n",
    "    threshold\n",
    "        The percentage of missing values that a feature must have to be considered incomplete.\n",
    "    verbose, optional\n",
    "        If True, prints the number of features in each category.\n",
    "    \n",
    "    '''\n",
    "    # Check the type and that the value of the `threshold`` parameter is in range: \n",
    "    if not isinstance(threshold, (float, int)):\n",
    "        # If invalid (not a float or an integer), than raise a TypeError. \n",
    "        raise TypeError()\n",
    "    elif (threshold < 0) or (threshold > 1):\n",
    "        # If invalid (not between 0 and 1), than raise a ValueError.\n",
    "        raise ValueError()\n",
    "\n",
    "    # Create a Series containing the percentage missing or null for each column in the dataset.\n",
    "    pct_incomplete = amex.isnull().sum().div(len(amex)).sort_values(ascending=False)\n",
    "\n",
    "    # Create set containing incomplete features, or column names where the percentage of missing or null values are greater than or equal to the assigned threshold.\n",
    "    incomplete_features = set(\n",
    "        pct_incomplete[pct_incomplete >= threshold].index.tolist()\n",
    "    )\n",
    "    # Print the number of incomplete features for a given threshold.\n",
    "    if verbose:\n",
    "        print(f\"Incomplete Features >= {threshold}%:\\n{incomplete_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_numerical_aggregates(amex_dataset):\n",
    "    '''Function to create new features by aggregating the dataset by [TODO: add content].\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    amex_dataset\n",
    "        The dataset that we want to create the aggregated features for.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        DataFrame of aggregated features derived from the original dataset.\n",
    "    \n",
    "    '''\n",
    "    # Select numeric features from the dataset\n",
    "    amex_numeric = amex_dataset.select_dtypes(include=\"number\")\n",
    "\n",
    "\n",
    "    # Create a new dataframe with the first and last rows of the grouped dataframe.\n",
    "    last_statement = amex_numeric.groupby(\"customer_ID\").nth(-1)\n",
    "    first_statement = amex_numeric.groupby(\"customer_ID\").nth(0)\n",
    "\n",
    "    # Divide the last statement by the first statement and filling the NaN values with 1.\n",
    "    lag_div = (last_statement\n",
    "               .div(first_statement)\n",
    "               .replace(-np.inf, 0)\n",
    "               .replace(np.inf, 2)\n",
    "               .fillna(1))\n",
    "\n",
    "    # Subtract the first statement from the last statement and filling the NaN values with 0.\n",
    "    lag_diff = (last_statement\n",
    "                .subtract(first_statement)\n",
    "                .replace([np.inf, -np.inf], np.nan)\n",
    "                .fillna(0))\n",
    "\n",
    "    # Rename column names by appending lad_type previous name.\n",
    "    lag_div.columns = [col + \"__lag_div\" for col in lag_div.columns]\n",
    "    lag_diff.columns = [col + \"__lag_diff\" for col in lag_diff.columns]\n",
    "\n",
    "    # Concatenate `lag_diff` and `lag_div` to create a new DataFrame with aggregated lag features.\n",
    "    numeric__agg_lag = pd.concat([lag_diff, lag_div], axis=1)\n",
    "\n",
    "    # Group columns by customer_ID and calculate aggregated features with statistical measures\n",
    "    numeric__agg = (amex_numeric\n",
    "                    .groupby(\"customer_ID\")\n",
    "                    .agg([\"last\", \"mean\", \"min\", \"max\", \"std\"]\n",
    "    ))\n",
    "    # Rename each column by joining the two levels of column names.\n",
    "    numeric__agg.columns = [\"__\".join(col) for col in numeric__agg.columns]\n",
    "\n",
    "    # Concatenate `numeric__agg` and `numeric__agg_lag` dataframes.\n",
    "    amex_agg = pd.concat([numeric__agg, numeric__agg_lag], axis=1)\n",
    "\n",
    "    # Returning the aggregated numerical features for the dataset.\n",
    "    return amex_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_categorical_aggregates(amex_dataset):\n",
    "    '''Function to create new features by aggregating the dataset by [TODO: add content].\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    amex_dataset\n",
    "        The dataset that we want to create the aggregated features for.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        DataFrame of aggregated features derived from the original dataset.\n",
    "    \n",
    "    '''\n",
    "    # Select numeric features from the dataset\n",
    "    amex_categorical = amex_dataset.select_dtypes(include=\"category\")   \n",
    "\n",
    "    # Group data by the customer ID; \n",
    "    # Select the columns with categorical data; \n",
    "    # Lastly, apply aggregate functions.\n",
    "    categorical__agg = (\n",
    "        amex_dataset\n",
    "        .groupby(\"customer_ID\")\n",
    "        .agg([\"first\", \"last\", \"count\", \"nunique\"])\n",
    "    )\n",
    "\n",
    "    # Rename each column by joining the two levels of column names.\n",
    "    categorical__agg.columns = [\"__\".join(col) for col in categorical__agg.columns]\n",
    "\n",
    "    return categorical__agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(amex_dataset):\n",
    "    '''> Function takes in the amex dataset as a DataFrame; drops columns that are invalid, creates aggregated features for the dataset, and returns a DataFrame with the aggregated features.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    amex_dataset\n",
    "        The dataset that we want to create the aggregated features for.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        The aggregated features for the dataset.\n",
    "    \n",
    "    '''\n",
    "    # Create a set consisting of incomplete features.\n",
    "    incomplete_features = {\n",
    "        'D_87', 'D_88', 'D_108', 'D_110', 'D_111', 'B_39', \n",
    "        'D_73', 'B_42', 'D_134', 'D_137', 'D_135', 'D_138', \n",
    "        'D_136', 'R_9', 'B_29', 'D_106', 'D_132', 'D_49', \n",
    "        'R_26', 'D_76', 'D_66', 'D_42'}\n",
    "\n",
    "    # Create a set of features that are now redundant.\n",
    "    made_redundant = {'S_2'}\n",
    "\n",
    "    # Create a set for the target variable.\n",
    "    target_variable = {'target'}\n",
    "\n",
    "    # Create a tuple of invalid features by taking the union of the three sets\n",
    "    invalid_cols = (incomplete_features | made_redundant | target_variable)\n",
    "\n",
    "    # Create a list of columns that are in both the `amex_dataset` and `invalid_cols`\n",
    "    cols_to_drop = (amex_dataset\n",
    "                    .columns\n",
    "                    .intersection(invalid_cols)\n",
    "                    .tolist())\n",
    "\n",
    "    # 1) Drop columns that in `cols_to_drop` from the dataset\n",
    "    # 2) Create aggregated features for the dataset.\n",
    "    numerical_aggregates = (amex_dataset\n",
    "                            .drop(cols_to_drop, axis=1)\n",
    "                            .pipe(create_numerical_aggregates))\n",
    "\n",
    "    categorical_aggregates = (amex_dataset\n",
    "                              .drop(cols_to_drop, axis=1)\n",
    "                              .pipe(create_categorical_aggregates))\n",
    "        \n",
    "    # Concatenating the `categorical__agg`, `numerical_aggregates` dataframes.\n",
    "    amex_aggregated = pd.concat([numerical_aggregates, categorical_aggregates], axis=1) \n",
    "\n",
    "    if 'target' in cols_to_drop:\n",
    "        # Add target variable to the DataFrame\n",
    "        amex_aggregated['target'] = (\n",
    "            amex_dataset.groupby('customer_ID').tail(1).target)\n",
    "    # Return DataFrame w/ aggregated features\n",
    "    return amex_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_datasets(read_feather=True, to_feather=True, to_csv=False):\n",
    "    '''Function loads the train and test datasets, makes features, and saves them to feather files\n",
    "    [TODO: add content]\n",
    "    Parameters\n",
    "    ----------\n",
    "    read_feather, optional\n",
    "        whether to read the feather files or not\n",
    "    to_feather, optional\n",
    "        whether to save the dataframe to feather format\n",
    "    to_csv, optional\n",
    "        If True, will save the dataframes to csv files.\n",
    "    \n",
    "    '''\n",
    "    # Load train and test datasets in feather format; pipe to make features for modeling.\n",
    "    train_agg = (load_dataset('train', use_feather=read_feather)\n",
    "                 .pipe(make_features))\n",
    "    test_agg = (load_dataset(\"test\", use_feather=read_feather)\n",
    "                .pipe(make_features))\n",
    "    \n",
    "    if to_feather:\n",
    "        # Save train_agg and test_agg datasets to disk as feather files.\n",
    "        train_agg.reset_index().to_feather('./data/interim/train_agg.ftr')\n",
    "        test_agg.reset_index().to_feather('./data/interim/test_agg.ftr')\n",
    "        \n",
    "    if to_csv:\n",
    "        # Save train_agg and test_agg datasets to disk as csv files.\n",
    "        train_agg.to_csv('./data/interim/train_agg.csv')\n",
    "        test_agg.to_csv('./data/interim/test_agg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_dict(X_train, verbose=True):\n",
    "    '''Function takes a dataset as a DataFrame and returns a dictionary of feature dictionaries.\n",
    "    [TODO: update content]\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train\n",
    "        The training dataset.\n",
    "    verbose, optional\n",
    "        If True, it will print the length of the numeric_features, categorical_features, and\n",
    "    ordinal_features.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        A dictionary with the keys and values of the numeric_features, categorical_features,\n",
    "    ordinal_features, and all_features.\n",
    "    \n",
    "    '''\n",
    "    # Select all numeric data within the dataset; access their column names; cast as list.\n",
    "    numeric_features = (X_train\n",
    "                        .select_dtypes(include='number')\n",
    "                        .columns\n",
    "                        .tolist())\n",
    "\n",
    "    # Select all categorical data within the dataset; access their column names; cast as list.\n",
    "    categorical_features = (X_train\n",
    "                            .select_dtypes(include='category')\n",
    "                            .columns\n",
    "                            .tolist())\n",
    "\n",
    "    # Create an empty list of ordinal features (for later use).\n",
    "    ordinal_features = []\n",
    "    \n",
    "    # Create list of all features by assigning the column names in the dataset to a list.\n",
    "    all_features = X_train.columns.tolist()\n",
    "    \n",
    "    if verbose:\n",
    "        #Print the number of features with numeric, categorical, or ordinal data.\n",
    "        print(f'Numeric Features - Count(#): {len(numeric_features)}')\n",
    "        print(f'Categorical Features - Count(#): {len(categorical_features)}')\n",
    "        print(f'Ordinal Features - Count(#): {len(ordinal_features)}')\n",
    "\n",
    "    # Return the dictionary of feature dictionaries\n",
    "    return {\n",
    "        'num': numeric_features,\n",
    "        'cat': categorical_features,\n",
    "        'ord': ordinal_features,\n",
    "        'all': all_features,\n",
    "        'numeric': numeric_features,\n",
    "        'categorical': categorical_features,\n",
    "        'ordinal': ordinal_features,\n",
    "        'all_features': all_features\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/external/train_data.ftr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/victor/amex-default-prediction/notebooks/amex-feature-engineering.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://vscode.naidu.one/home/victor/amex-default-prediction/notebooks/amex-feature-engineering.ipynb#ch0000009vscode-remote?line=0'>1</a>\u001b[0m dataset \u001b[39m=\u001b[39m load_dataset(\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m, use_feather\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;32m/home/victor/amex-default-prediction/notebooks/amex-feature-engineering.ipynb Cell 3'\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(dataset, use_feather)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode.naidu.one/home/victor/amex-default-prediction/notebooks/amex-feature-engineering.ipynb#ch0000002vscode-remote?line=51'>52</a>\u001b[0m     feather_file \u001b[39m=\u001b[39m data_filepaths[\u001b[39m\"\u001b[39m\u001b[39mfeather\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mget(dataset)\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode.naidu.one/home/victor/amex-default-prediction/notebooks/amex-feature-engineering.ipynb#ch0000002vscode-remote?line=52'>53</a>\u001b[0m     \u001b[39m# Return feather file as Pandas DataFrame; set index to the customer ID.\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://vscode.naidu.one/home/victor/amex-default-prediction/notebooks/amex-feature-engineering.ipynb#ch0000002vscode-remote?line=53'>54</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m pd\u001b[39m.\u001b[39;49mread_feather(feather_file)\u001b[39m.\u001b[39mset_index(\u001b[39m\"\u001b[39m\u001b[39mcustomer_ID\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode.naidu.one/home/victor/amex-default-prediction/notebooks/amex-feature-engineering.ipynb#ch0000002vscode-remote?line=54'>55</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode.naidu.one/home/victor/amex-default-prediction/notebooks/amex-feature-engineering.ipynb#ch0000002vscode-remote?line=55'>56</a>\u001b[0m     \u001b[39m# Set path to *.csv file for the dataset.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode.naidu.one/home/victor/amex-default-prediction/notebooks/amex-feature-engineering.ipynb#ch0000002vscode-remote?line=56'>57</a>\u001b[0m     csv_file \u001b[39m=\u001b[39m data_filepaths[\u001b[39m\"\u001b[39m\u001b[39mcsv\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mget(dataset)\n",
      "File \u001b[0;32m~/anaconda3/envs/amex_v2/lib/python3.10/site-packages/pandas/io/feather_format.py:128\u001b[0m, in \u001b[0;36mread_feather\u001b[0;34m(path, columns, use_threads, storage_options)\u001b[0m\n\u001b[1;32m    125\u001b[0m import_optional_dependency(\u001b[39m\"\u001b[39m\u001b[39mpyarrow\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    126\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyarrow\u001b[39;00m \u001b[39mimport\u001b[39;00m feather\n\u001b[0;32m--> 128\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n\u001b[1;32m    129\u001b[0m     path, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m, storage_options\u001b[39m=\u001b[39;49mstorage_options, is_text\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[1;32m    130\u001b[0m ) \u001b[39mas\u001b[39;00m handles:\n\u001b[1;32m    132\u001b[0m     \u001b[39mreturn\u001b[39;00m feather\u001b[39m.\u001b[39mread_feather(\n\u001b[1;32m    133\u001b[0m         handles\u001b[39m.\u001b[39mhandle, columns\u001b[39m=\u001b[39mcolumns, use_threads\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m(use_threads)\n\u001b[1;32m    134\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/amex_v2/lib/python3.10/site-packages/pandas/io/common.py:795\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    786\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\n\u001b[1;32m    787\u001b[0m             handle,\n\u001b[1;32m    788\u001b[0m             ioargs\u001b[39m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    791\u001b[0m             newline\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    792\u001b[0m         )\n\u001b[1;32m    793\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    794\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 795\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(handle, ioargs\u001b[39m.\u001b[39;49mmode)\n\u001b[1;32m    796\u001b[0m     handles\u001b[39m.\u001b[39mappend(handle)\n\u001b[1;32m    798\u001b[0m \u001b[39m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/external/train_data.ftr'"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('train', use_feather=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_features = ['P_2', 'D_48', 'R_1', 'S_3', 'B_7']\n",
    "num_agg = create_numerical_aggregates(dataset[key_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3.10.5 ('amex_v2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "790edd3242a40ac70c58114a122b744731b030acc406feafcec626ff57999bc9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
