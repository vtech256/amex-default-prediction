{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import (\n",
    "    LabelEncoder,\n",
    "    OneHotEncoder,\n",
    "    OrdinalEncoder,\n",
    "    RobustScaler,\n",
    "    StandardScaler,\n",
    ")\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer\n",
    "from sklearn.feature_selection import (\n",
    "    SelectFromModel,\n",
    "    SelectKBest,\n",
    "    SequentialFeatureSelector,\n",
    "    VarianceThreshold,\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change working directory to project root\n",
    "if os.getcwd().split(\"/\")[-1] == \"notebooks\":\n",
    "    # Change dir to parent directory.\n",
    "    os.chdir(\"../\")\n",
    "    # Print the current working directory\n",
    "    print(f'Current Dir: {os.getcwd()}')\n",
    "    \n",
    "# Enable garbage collection\n",
    "gc.enable()\n",
    "\n",
    "# > Configure display options for Pandas\n",
    "# ---------------------------------------------------------------\n",
    "# Set display width to 1000.\n",
    "pd.set_option(\"display.width\", 1000)\n",
    "# Set maximum number of rows to display to 500.\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "# Set maximum number of columns to display to 500.\n",
    "pd.set_option(\"display.max_columns\", 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run notebooks/amex-feature-engineering.ipynb import load_dataset, features_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "lines_to_end_of_cell_marker": 2,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/kyakovlev\n",
    "# https://www.kaggle.com/competitions/amex-default-prediction/discussion/327534\n",
    "def amex_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    > The function takes in the true labels and the predicted labels, and returns the evaluation  score. (TODO: correct the following passage) The evaluation metric is the average of the Gini coefficient and the top 4% score\n",
    "\n",
    "    Args:\n",
    "      y_true: the true labels\n",
    "      y_pred: the predicted probabilities of the positive class\n",
    "\n",
    "    Returns:\n",
    "      the score of the model.\n",
    "    \"\"\"\n",
    "    labels = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels = labels[labels[:, 1].argsort()[::-1]]\n",
    "    weights = np.where(labels[:, 0] == 0, 20, 1)\n",
    "    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four = np.sum(cut_vals[:, 0]) / np.sum(labels[:, 0])\n",
    "\n",
    "    gini = [0, 0]\n",
    "    for i in [1, 0]:\n",
    "        labels = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight = np.where(labels[:, 0] == 0, 20, 1)\n",
    "        weight_random = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos = np.sum(labels[:, 0] * weight)\n",
    "        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz = cum_pos_found / total_pos\n",
    "        gini[i] = np.sum((lorentz - weight_random) * weight)\n",
    "\n",
    "    # Returning the average of the Gini coefficient and the top 4% score.\n",
    "    return 0.5 * (gini[1] / gini[0] + top_four)\n",
    "\n",
    "\n",
    "def xgb__amex_metric(labels, predt):\n",
    "    \"\"\"\n",
    "    It takes in the actual values and the predicted values and returns the score\n",
    "\n",
    "    Args:\n",
    "      labels: the actual values of the target variable\n",
    "      predt: the predictions from the model\n",
    "\n",
    "    Returns:\n",
    "      the score of the model.\n",
    "    \"\"\"\n",
    "    score = 1 - amex_score(labels, predt)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_preprocessor(X_train):\n",
    "    \"\"\"\n",
    "    It takes the training dataset as input and returns a pipeline that can be used to\n",
    "    preprocess the training data and the test data.\n",
    "\n",
    "    Args:\n",
    "      X_train: The training data\n",
    "\n",
    "    Returns:\n",
    "      A pipeline object\n",
    "    \"\"\"\n",
    "    # Creating a dictionary of the features in the training set.\n",
    "    features = features_dict(X_train)\n",
    "\n",
    "    # Impute missing values in numeric features with their median values and then adding an indicator column to indicate which values were imputed.\n",
    "    numeric_preprocessor = make_pipeline(\n",
    "        SimpleImputer(strategy=\"median\", add_indicator=True)\n",
    "    )\n",
    "\n",
    "    # Impute missing values in categorical features with their most frequent value before one-hot-encoding the categorical features.\n",
    "    categorical_preprocessor = make_pipeline(\n",
    "        SimpleImputer(strategy=\"most_frequent\", add_indicator=True),\n",
    "        OneHotEncoder(handle_unknown=\"ignore\", sparse=False),\n",
    "    )\n",
    "\n",
    "    # Impute missing values in ordinal features with -1 and then encoding the ordinal features.\n",
    "    ordinal_preprocessor = make_pipeline(\n",
    "        SimpleImputer(strategy=\"constant\", fill_value=-1, add_indicator=True),\n",
    "        OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-11),\n",
    "    )\n",
    "\n",
    "    # Make ColumnTransformer to combine various numeric/categorical transformers\n",
    "    feature_preprocessor = ColumnTransformer(\n",
    "        [\n",
    "            (\"numeric\", numeric_preprocessor, features[\"numeric\"]),\n",
    "            (\"categorical\", categorical_preprocessor, features[\"categorical\"]),\n",
    "        ],\n",
    "        verbose_feature_names_out=True,\n",
    "    )\n",
    "\n",
    "    # Select the top 750 features from the training set using the Random Forest Classifier.\n",
    "    feature_selector = SelectFromModel(\n",
    "        RandomForestClassifier(n_estimators=25, random_state=1123), max_features=750\n",
    "    )\n",
    "\n",
    "    # Make the final pipeline to preprocess the training and test datasets.\n",
    "    preprocessor_pipeline = make_pipeline(feature_preprocessor)\n",
    "\n",
    "    # Return the pipeline object.\n",
    "    return preprocessor_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Version: 1.6.1\n"
     ]
    }
   ],
   "source": [
    "# Print version of XGBoost used\n",
    "print(f\"XGB Version: {xgb.__version__}\")\n",
    "\n",
    "# Instantiate the XGBClassifier\n",
    "xgb_clf = xgb.XGBClassifier(\n",
<<<<<<< HEAD
    "    objective='binary:logistic',\n",
    "    num_parallel_tree=12,\n",
    "    tree_method='gpu_hist',\n",
    "    booster='dart',\n",
    "    grow_policy='lossguide',\n",
=======
    "    objective=\"binary:logistic\",\n",
    "    booster=\"dart\",\n",
>>>>>>> b6ad3873d4d82e9edf81f528274c8286fd47f5ab
    "    use_label_encoder=False,\n",
    "    max_depth=5,\n",
    "    subsample=0.88,\n",
    "    colsample_bytree=0.72,\n",
    "    custom_metric=amex_score, \n",
    "    early_stopping_rounds=5,\n",
    "    n_estimators=128,\n",
    "    learning_rate=0.32,\n",
    "    feval=amex_score,\n",
    "    eval_metric=xgb__amex_metric,\n",
    "    verbosity=3,\n",
    "    seed=1123,\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data from the `data/processed` directory.\n",
    "amex_train__agg = load_dataset(\"train_agg\", use_feather=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data into train and test sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    amex_train__agg.drop(\"target\", axis=1),\n",
    "    amex_train__agg.target,\n",
    "    stratify=amex_train__agg.target,\n",
    "    test_size=0.20,\n",
    "    random_state=1123,\n",
    ")\n",
    "\n",
    "# Delete the `amex_train__agg` variable and then call the garbage collector to free up memory.\n",
    "del amex_train__agg\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric Features - Count(#): 1424\n",
      "Categorical Features - Count(#): 20\n",
      "Ordinal Features - Count(#): 0\n"
     ]
    }
   ],
   "source": [
    "# Make a pipeline object that can be used to preprocess the training and test datasets.\n",
    "feature_preprocessor = make_preprocessor(X_train)\n",
    "\n",
    "# Fit the preprocessor to the training data.\n",
    "feature_preprocessor.fit(X_train, y_train)\n",
    "\n",
    "# Preprocess the training and test datasets.\n",
    "X_train__preprocessed = feature_preprocessor.transform(X_train)\n",
    "X_test__preprocessed = feature_preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "[15:50:45] /home/conda/feedstock_root/build_artifacts/xgboost-split_1660208814268/work/src/data/../common/common.h:239: XGBoost version not compiled with GPU support.\nStack trace:\n  [bt] (0) /home/victor/anaconda3/envs/amex_v2/lib/libxgboost.so(+0xb3c3f) [0x7fa2cbfbbc3f]\n  [bt] (1) /home/victor/anaconda3/envs/amex_v2/lib/libxgboost.so(+0x171c22) [0x7fa2cc079c22]\n  [bt] (2) /home/victor/anaconda3/envs/amex_v2/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0xdd) [0x7fa2cc10118d]\n  [bt] (3) /home/victor/anaconda3/envs/amex_v2/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x329) [0x7fa2cc117809]\n  [bt] (4) /home/victor/anaconda3/envs/amex_v2/lib/libxgboost.so(+0x21011d) [0x7fa2cc11811d]\n  [bt] (5) /home/victor/anaconda3/envs/amex_v2/lib/libxgboost.so(+0x23ddbe) [0x7fa2cc145dbe]\n  [bt] (6) /home/victor/anaconda3/envs/amex_v2/lib/libxgboost.so(XGBoosterBoostedRounds+0x2d) [0x7fa2cbfbdd4d]\n  [bt] (7) /home/victor/anaconda3/envs/amex_v2/lib/python3.10/lib-dynload/../../libffi.so.8(+0x6a4a) [0x7fa32fe27a4a]\n  [bt] (8) /home/victor/anaconda3/envs/amex_v2/lib/python3.10/lib-dynload/../../libffi.so.8(+0x5fea) [0x7fa32fe26fea]\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/victor/amex-default-prediction/notebooks/amex-machine-learning.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://vscode.naidu.one/home/victor/amex-default-prediction/notebooks/amex-machine-learning.ipynb#ch0000009vscode-remote?line=0'>1</a>\u001b[0m \u001b[39m# Fit the classifier to the training set\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://vscode.naidu.one/home/victor/amex-default-prediction/notebooks/amex-machine-learning.ipynb#ch0000009vscode-remote?line=1'>2</a>\u001b[0m xgb_clf\u001b[39m.\u001b[39;49mfit(X_train__preprocessed, y_train, \n\u001b[1;32m      <a href='vscode-notebook-cell://vscode.naidu.one/home/victor/amex-default-prediction/notebooks/amex-machine-learning.ipynb#ch0000009vscode-remote?line=2'>3</a>\u001b[0m             eval_set\u001b[39m=\u001b[39;49m[(X_test__preprocessed, y_test)])\n",
      "File \u001b[0;32m~/anaconda3/envs/amex_v2/lib/python3.10/site-packages/xgboost/core.py:532\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    531\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 532\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/amex_v2/lib/python3.10/site-packages/xgboost/sklearn.py:1400\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1379\u001b[0m model, metric, params, early_stopping_rounds, callbacks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_configure_fit(\n\u001b[1;32m   1380\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[1;32m   1381\u001b[0m )\n\u001b[1;32m   1382\u001b[0m train_dmatrix, evals \u001b[39m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m   1383\u001b[0m     missing\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmissing,\n\u001b[1;32m   1384\u001b[0m     X\u001b[39m=\u001b[39mX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1397\u001b[0m     enable_categorical\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menable_categorical,\n\u001b[1;32m   1398\u001b[0m )\n\u001b[0;32m-> 1400\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[1;32m   1401\u001b[0m     params,\n\u001b[1;32m   1402\u001b[0m     train_dmatrix,\n\u001b[1;32m   1403\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_num_boosting_rounds(),\n\u001b[1;32m   1404\u001b[0m     evals\u001b[39m=\u001b[39;49mevals,\n\u001b[1;32m   1405\u001b[0m     early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds,\n\u001b[1;32m   1406\u001b[0m     evals_result\u001b[39m=\u001b[39;49mevals_result,\n\u001b[1;32m   1407\u001b[0m     obj\u001b[39m=\u001b[39;49mobj,\n\u001b[1;32m   1408\u001b[0m     custom_metric\u001b[39m=\u001b[39;49mmetric,\n\u001b[1;32m   1409\u001b[0m     verbose_eval\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   1410\u001b[0m     xgb_model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   1411\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m   1412\u001b[0m )\n\u001b[1;32m   1414\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m callable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective):\n\u001b[1;32m   1415\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective \u001b[39m=\u001b[39m params[\u001b[39m\"\u001b[39m\u001b[39mobjective\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/amex_v2/lib/python3.10/site-packages/xgboost/core.py:532\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    531\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 532\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/amex_v2/lib/python3.10/site-packages/xgboost/training.py:176\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    164\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(\n\u001b[1;32m    165\u001b[0m         callback\u001b[39m.\u001b[39mEarlyStopping(rounds\u001b[39m=\u001b[39mearly_stopping_rounds, maximize\u001b[39m=\u001b[39mmaximize)\n\u001b[1;32m    166\u001b[0m     )\n\u001b[1;32m    167\u001b[0m cb_container \u001b[39m=\u001b[39m callback\u001b[39m.\u001b[39mCallbackContainer(\n\u001b[1;32m    168\u001b[0m     callbacks,\n\u001b[1;32m    169\u001b[0m     metric\u001b[39m=\u001b[39mmetric_fn,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m     output_margin\u001b[39m=\u001b[39mcallable(obj) \u001b[39mor\u001b[39;00m metric_fn \u001b[39mis\u001b[39;00m feval,\n\u001b[1;32m    174\u001b[0m )\n\u001b[0;32m--> 176\u001b[0m bst \u001b[39m=\u001b[39m cb_container\u001b[39m.\u001b[39;49mbefore_training(bst)\n\u001b[1;32m    178\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(start_iteration, num_boost_round):\n\u001b[1;32m    179\u001b[0m     \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n",
      "File \u001b[0;32m~/anaconda3/envs/amex_v2/lib/python3.10/site-packages/xgboost/callback.py:147\u001b[0m, in \u001b[0;36mCallbackContainer.before_training\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39m'''Function called before training.'''\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[0;32m--> 147\u001b[0m     model \u001b[39m=\u001b[39m c\u001b[39m.\u001b[39;49mbefore_training(model\u001b[39m=\u001b[39;49mmodel)\n\u001b[1;32m    148\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbefore_training should return the model\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    149\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_cv:\n",
      "File \u001b[0;32m~/anaconda3/envs/amex_v2/lib/python3.10/site-packages/xgboost/callback.py:349\u001b[0m, in \u001b[0;36mEarlyStopping.before_training\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbefore_training\u001b[39m(\u001b[39mself\u001b[39m, model):\n\u001b[0;32m--> 349\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstarting_round \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mnum_boosted_rounds()\n\u001b[1;32m    350\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/anaconda3/envs/amex_v2/lib/python3.10/site-packages/xgboost/core.py:2274\u001b[0m, in \u001b[0;36mBooster.num_boosted_rounds\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2272\u001b[0m rounds \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39mc_int()\n\u001b[1;32m   2273\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2274\u001b[0m _check_call(_LIB\u001b[39m.\u001b[39;49mXGBoosterBoostedRounds(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle, ctypes\u001b[39m.\u001b[39;49mbyref(rounds)))\n\u001b[1;32m   2275\u001b[0m \u001b[39mreturn\u001b[39;00m rounds\u001b[39m.\u001b[39mvalue\n",
      "File \u001b[0;32m~/anaconda3/envs/amex_v2/lib/python3.10/site-packages/xgboost/core.py:203\u001b[0m, in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[39m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \n\u001b[1;32m    194\u001b[0m \u001b[39mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[39m    return value from API calls\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 203\u001b[0m     \u001b[39mraise\u001b[39;00m XGBoostError(py_str(_LIB\u001b[39m.\u001b[39mXGBGetLastError()))\n",
      "\u001b[0;31mXGBoostError\u001b[0m: [15:50:45] /home/conda/feedstock_root/build_artifacts/xgboost-split_1660208814268/work/src/data/../common/common.h:239: XGBoost version not compiled with GPU support.\nStack trace:\n  [bt] (0) /home/victor/anaconda3/envs/amex_v2/lib/libxgboost.so(+0xb3c3f) [0x7fa2cbfbbc3f]\n  [bt] (1) /home/victor/anaconda3/envs/amex_v2/lib/libxgboost.so(+0x171c22) [0x7fa2cc079c22]\n  [bt] (2) /home/victor/anaconda3/envs/amex_v2/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0xdd) [0x7fa2cc10118d]\n  [bt] (3) /home/victor/anaconda3/envs/amex_v2/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x329) [0x7fa2cc117809]\n  [bt] (4) /home/victor/anaconda3/envs/amex_v2/lib/libxgboost.so(+0x21011d) [0x7fa2cc11811d]\n  [bt] (5) /home/victor/anaconda3/envs/amex_v2/lib/libxgboost.so(+0x23ddbe) [0x7fa2cc145dbe]\n  [bt] (6) /home/victor/anaconda3/envs/amex_v2/lib/libxgboost.so(XGBoosterBoostedRounds+0x2d) [0x7fa2cbfbdd4d]\n  [bt] (7) /home/victor/anaconda3/envs/amex_v2/lib/python3.10/lib-dynload/../../libffi.so.8(+0x6a4a) [0x7fa32fe27a4a]\n  [bt] (8) /home/victor/anaconda3/envs/amex_v2/lib/python3.10/lib-dynload/../../libffi.so.8(+0x5fea) [0x7fa32fe26fea]\n\n"
     ]
    }
   ],
   "source": [
    "# Fit the classifier to the training set\n",
    "xgb_clf.fit(X_train__preprocessed, y_train, eval_set=[(X_test__preprocessed, y_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the probabilities for the positive class in the training and test datasets.\n",
    "train_preds = xgb_clf.predict_proba(X_train__preprocessed)[:, 1]\n",
    "test_preds = xgb_clf.predict_proba(X_test__preprocessed)[:, 1]\n",
    "\n",
    "# Calculate the evaluation metric for the training and test datasets.\n",
    "train_score = amex_score(y_train.values, train_preds)\n",
    "test_score = amex_score(y_test.values, test_preds)\n",
    "\n",
    "# Print model scores for both the training and test datasets.\n",
    "print(f\"Train Score: {train_score}\")\n",
    "print(f\"Test Score: {test_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the `feature_preprocessor` object to a file called `feature_preprocessor.pkl` in the `models` directory.\n",
    "with open(\"models/feature_preprocessor.pkl\", \"wb\") as f:\n",
    "    pickle.dump(feature_preprocessor, f)\n",
    "\n",
<<<<<<< HEAD
    "with open('models/xgb_clf2.pkl','wb') as f:\n",
=======
    "# Save the `xgb_clf` object to a file called `xgb_clf.pkl` in the `models` directory.\n",
    "with open(\"models/xgb_clf.pkl\", \"wb\") as f:\n",
>>>>>>> b6ad3873d4d82e9edf81f528274c8286fd47f5ab
    "    pickle.dump(xgb_clf, f)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "28ed4e3bfc6dbc0ee6f9216b9beca2c6fff623d080b9c5b8a874eb933f3db8ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
