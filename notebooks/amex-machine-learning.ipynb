{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder, RobustScaler, StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer\n",
    "from sklearn.feature_selection import SelectFromModel, SelectKBest, SequentialFeatureSelector, VarianceThreshold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change working directory to project root\n",
    "if os.getcwd().split(\"/\")[-1] == \"notebooks\":\n",
    "    # Change working dir to project root\n",
    "    os.chdir(\"../\")\n",
    "    \n",
    "    # Print the current working directory\n",
    "    # print(f'Current Dir: {os.getcwd()}')\n",
    "    \n",
    "# Enable garbage collection\n",
    "gc.enable()\n",
    "\n",
    "# Configure display options for Pandas\n",
    "# (*) Helpful when displaying DFs w/ numerous features\n",
    "pd.set_option(\"display.width\", 1000)\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run notebooks/amex-feature-engineering.ipynb import load_dataset, features_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/kyakovlev\n",
    "# https://www.kaggle.com/competitions/amex-default-prediction/discussion/327534\n",
    "def amex_score(y_true, y_pred):\n",
    "\n",
    "    labels     = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels     = labels[labels[:, 1].argsort()[::-1]]\n",
    "    weights    = np.where(labels[:,0]==0, 20, 1)\n",
    "    cut_vals   = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four   = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "\n",
    "    gini = [0,0]\n",
    "    for i in [1,0]:\n",
    "        labels         = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels         = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight         = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random  = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos      = np.sum(labels[:, 0] *  weight)\n",
    "        cum_pos_found  = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz        = cum_pos_found / total_pos\n",
    "        gini[i]        = np.sum((lorentz - weight_random) * weight)\n",
    "\n",
    "    return 0.5 * (gini[1]/gini[0] + top_four)\n",
    "\n",
    "\n",
    "def xgb__amex_metric(labels, predt):\n",
    "    score = 1 - amex_score(labels, predt)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_preprocessor(X_train):\n",
    "    features = features_dict(X_train)\n",
    "\n",
    "    numeric_preprocessor = make_pipeline(\n",
    "        SimpleImputer(strategy='median', add_indicator=True))\n",
    "\n",
    "    categorical_preprocessor = make_pipeline(\n",
    "        SimpleImputer(strategy='most_frequent', add_indicator=True),\n",
    "        OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "\n",
    "    ordinal_preprocessor = make_pipeline(\n",
    "        SimpleImputer(strategy='constant', fill_value=-1, add_indicator=True),\n",
    "        OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-11))\n",
    "\n",
    "    feature_preprocessor = ColumnTransformer([\n",
    "            ('numeric', numeric_preprocessor, features['numeric']),\n",
    "            ('categorical', categorical_preprocessor, features['categorical'])\n",
    "        ], verbose_feature_names_out=True)\n",
    "\n",
    "    feature_selector = SelectFromModel(\n",
    "        RandomForestClassifier(\n",
    "            n_estimators=25,\n",
    "            random_state=1123), \n",
    "        max_features=750)\n",
    "\n",
    "    preprocessor_pipeline = make_pipeline(\n",
    "        feature_preprocessor)\n",
    "    \n",
    "    return preprocessor_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Version: 1.6.1\n"
     ]
    }
   ],
   "source": [
    "# Print version of XGBoost used\n",
    "print(f'XGB Version: {xgb.__version__}')\n",
    "\n",
    "# Instantiate the XGBClassifier\n",
    "xgb_clf = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    num_parallel_tree=12,\n",
    "    tree_method='gpu_hist',\n",
    "    booster='dart',\n",
    "    grow_policy='lossguide',\n",
    "    use_label_encoder=False,\n",
    "    max_depth=5,\n",
    "    subsample=0.88,\n",
    "    colsample_bytree=0.72,\n",
    "    custom_metric=amex_score, \n",
    "    early_stopping_rounds=5,\n",
    "    n_estimators=128,\n",
    "    learning_rate=0.32,\n",
    "    feval=amex_score,\n",
    "    eval_metric=xgb__amex_metric,\n",
    "    verbosity=3,\n",
    "    seed=1123, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "amex_train__agg = load_dataset('train_agg', use_feather=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    amex_train__agg.drop('target', axis=1), \n",
    "    amex_train__agg.target,\n",
    "    stratify=amex_train__agg.target,\n",
    "    test_size=0.20,\n",
    "    random_state=1123)\n",
    "\n",
    "del amex_train__agg\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric Features - Count(#): 1424\n",
      "Categorical Features - Count(#): 20\n",
      "Ordinal Features - Count(#): 0\n"
     ]
    }
   ],
   "source": [
    "feature_preprocessor = make_preprocessor(X_train)\n",
    "\n",
    "feature_preprocessor.fit(X_train, y_train)\n",
    "\n",
    "X_train__preprocessed = feature_preprocessor.transform(X_train)\n",
    "X_test__preprocessed = feature_preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "[15:50:45] /home/conda/feedstock_root/build_artifacts/xgboost-split_1660208814268/work/src/data/../common/common.h:239: XGBoost version not compiled with GPU support.\nStack trace:\n  [bt] (0) /home/victor/anaconda3/envs/amex_v2/lib/libxgboost.so(+0xb3c3f) [0x7fa2cbfbbc3f]\n  [bt] (1) /home/victor/anaconda3/envs/amex_v2/lib/libxgboost.so(+0x171c22) [0x7fa2cc079c22]\n  [bt] (2) /home/victor/anaconda3/envs/amex_v2/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0xdd) [0x7fa2cc10118d]\n  [bt] (3) /home/victor/anaconda3/envs/amex_v2/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x329) [0x7fa2cc117809]\n  [bt] (4) /home/victor/anaconda3/envs/amex_v2/lib/libxgboost.so(+0x21011d) [0x7fa2cc11811d]\n  [bt] (5) /home/victor/anaconda3/envs/amex_v2/lib/libxgboost.so(+0x23ddbe) [0x7fa2cc145dbe]\n  [bt] (6) /home/victor/anaconda3/envs/amex_v2/lib/libxgboost.so(XGBoosterBoostedRounds+0x2d) [0x7fa2cbfbdd4d]\n  [bt] (7) /home/victor/anaconda3/envs/amex_v2/lib/python3.10/lib-dynload/../../libffi.so.8(+0x6a4a) [0x7fa32fe27a4a]\n  [bt] (8) /home/victor/anaconda3/envs/amex_v2/lib/python3.10/lib-dynload/../../libffi.so.8(+0x5fea) [0x7fa32fe26fea]\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/victor/amex-default-prediction/notebooks/amex-machine-learning.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://vscode.naidu.one/home/victor/amex-default-prediction/notebooks/amex-machine-learning.ipynb#ch0000009vscode-remote?line=0'>1</a>\u001b[0m \u001b[39m# Fit the classifier to the training set\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://vscode.naidu.one/home/victor/amex-default-prediction/notebooks/amex-machine-learning.ipynb#ch0000009vscode-remote?line=1'>2</a>\u001b[0m xgb_clf\u001b[39m.\u001b[39;49mfit(X_train__preprocessed, y_train, \n\u001b[1;32m      <a href='vscode-notebook-cell://vscode.naidu.one/home/victor/amex-default-prediction/notebooks/amex-machine-learning.ipynb#ch0000009vscode-remote?line=2'>3</a>\u001b[0m             eval_set\u001b[39m=\u001b[39;49m[(X_test__preprocessed, y_test)])\n",
      "File \u001b[0;32m~/anaconda3/envs/amex_v2/lib/python3.10/site-packages/xgboost/core.py:532\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    531\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 532\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/amex_v2/lib/python3.10/site-packages/xgboost/sklearn.py:1400\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1379\u001b[0m model, metric, params, early_stopping_rounds, callbacks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_configure_fit(\n\u001b[1;32m   1380\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[1;32m   1381\u001b[0m )\n\u001b[1;32m   1382\u001b[0m train_dmatrix, evals \u001b[39m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m   1383\u001b[0m     missing\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmissing,\n\u001b[1;32m   1384\u001b[0m     X\u001b[39m=\u001b[39mX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1397\u001b[0m     enable_categorical\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menable_categorical,\n\u001b[1;32m   1398\u001b[0m )\n\u001b[0;32m-> 1400\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[1;32m   1401\u001b[0m     params,\n\u001b[1;32m   1402\u001b[0m     train_dmatrix,\n\u001b[1;32m   1403\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_num_boosting_rounds(),\n\u001b[1;32m   1404\u001b[0m     evals\u001b[39m=\u001b[39;49mevals,\n\u001b[1;32m   1405\u001b[0m     early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds,\n\u001b[1;32m   1406\u001b[0m     evals_result\u001b[39m=\u001b[39;49mevals_result,\n\u001b[1;32m   1407\u001b[0m     obj\u001b[39m=\u001b[39;49mobj,\n\u001b[1;32m   1408\u001b[0m     custom_metric\u001b[39m=\u001b[39;49mmetric,\n\u001b[1;32m   1409\u001b[0m     verbose_eval\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   1410\u001b[0m     xgb_model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   1411\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m   1412\u001b[0m )\n\u001b[1;32m   1414\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m callable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective):\n\u001b[1;32m   1415\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective \u001b[39m=\u001b[39m params[\u001b[39m\"\u001b[39m\u001b[39mobjective\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/amex_v2/lib/python3.10/site-packages/xgboost/core.py:532\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    531\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 532\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/amex_v2/lib/python3.10/site-packages/xgboost/training.py:176\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    164\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(\n\u001b[1;32m    165\u001b[0m         callback\u001b[39m.\u001b[39mEarlyStopping(rounds\u001b[39m=\u001b[39mearly_stopping_rounds, maximize\u001b[39m=\u001b[39mmaximize)\n\u001b[1;32m    166\u001b[0m     )\n\u001b[1;32m    167\u001b[0m cb_container \u001b[39m=\u001b[39m callback\u001b[39m.\u001b[39mCallbackContainer(\n\u001b[1;32m    168\u001b[0m     callbacks,\n\u001b[1;32m    169\u001b[0m     metric\u001b[39m=\u001b[39mmetric_fn,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m     output_margin\u001b[39m=\u001b[39mcallable(obj) \u001b[39mor\u001b[39;00m metric_fn \u001b[39mis\u001b[39;00m feval,\n\u001b[1;32m    174\u001b[0m )\n\u001b[0;32m--> 176\u001b[0m bst \u001b[39m=\u001b[39m cb_container\u001b[39m.\u001b[39;49mbefore_training(bst)\n\u001b[1;32m    178\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(start_iteration, num_boost_round):\n\u001b[1;32m    179\u001b[0m     \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n",
      "File \u001b[0;32m~/anaconda3/envs/amex_v2/lib/python3.10/site-packages/xgboost/callback.py:147\u001b[0m, in \u001b[0;36mCallbackContainer.before_training\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39m'''Function called before training.'''\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[0;32m--> 147\u001b[0m     model \u001b[39m=\u001b[39m c\u001b[39m.\u001b[39;49mbefore_training(model\u001b[39m=\u001b[39;49mmodel)\n\u001b[1;32m    148\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbefore_training should return the model\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    149\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_cv:\n",
      "File \u001b[0;32m~/anaconda3/envs/amex_v2/lib/python3.10/site-packages/xgboost/callback.py:349\u001b[0m, in \u001b[0;36mEarlyStopping.before_training\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbefore_training\u001b[39m(\u001b[39mself\u001b[39m, model):\n\u001b[0;32m--> 349\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstarting_round \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mnum_boosted_rounds()\n\u001b[1;32m    350\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/anaconda3/envs/amex_v2/lib/python3.10/site-packages/xgboost/core.py:2274\u001b[0m, in \u001b[0;36mBooster.num_boosted_rounds\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2272\u001b[0m rounds \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39mc_int()\n\u001b[1;32m   2273\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2274\u001b[0m _check_call(_LIB\u001b[39m.\u001b[39;49mXGBoosterBoostedRounds(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle, ctypes\u001b[39m.\u001b[39;49mbyref(rounds)))\n\u001b[1;32m   2275\u001b[0m \u001b[39mreturn\u001b[39;00m rounds\u001b[39m.\u001b[39mvalue\n",
      "File \u001b[0;32m~/anaconda3/envs/amex_v2/lib/python3.10/site-packages/xgboost/core.py:203\u001b[0m, in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[39m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \n\u001b[1;32m    194\u001b[0m \u001b[39mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[39m    return value from API calls\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 203\u001b[0m     \u001b[39mraise\u001b[39;00m XGBoostError(py_str(_LIB\u001b[39m.\u001b[39mXGBGetLastError()))\n",
      "\u001b[0;31mXGBoostError\u001b[0m: [15:50:45] /home/conda/feedstock_root/build_artifacts/xgboost-split_1660208814268/work/src/data/../common/common.h:239: XGBoost version not compiled with GPU support.\nStack trace:\n  [bt] (0) /home/victor/anaconda3/envs/amex_v2/lib/libxgboost.so(+0xb3c3f) [0x7fa2cbfbbc3f]\n  [bt] (1) /home/victor/anaconda3/envs/amex_v2/lib/libxgboost.so(+0x171c22) [0x7fa2cc079c22]\n  [bt] (2) /home/victor/anaconda3/envs/amex_v2/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0xdd) [0x7fa2cc10118d]\n  [bt] (3) /home/victor/anaconda3/envs/amex_v2/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x329) [0x7fa2cc117809]\n  [bt] (4) /home/victor/anaconda3/envs/amex_v2/lib/libxgboost.so(+0x21011d) [0x7fa2cc11811d]\n  [bt] (5) /home/victor/anaconda3/envs/amex_v2/lib/libxgboost.so(+0x23ddbe) [0x7fa2cc145dbe]\n  [bt] (6) /home/victor/anaconda3/envs/amex_v2/lib/libxgboost.so(XGBoosterBoostedRounds+0x2d) [0x7fa2cbfbdd4d]\n  [bt] (7) /home/victor/anaconda3/envs/amex_v2/lib/python3.10/lib-dynload/../../libffi.so.8(+0x6a4a) [0x7fa32fe27a4a]\n  [bt] (8) /home/victor/anaconda3/envs/amex_v2/lib/python3.10/lib-dynload/../../libffi.so.8(+0x5fea) [0x7fa32fe26fea]\n\n"
     ]
    }
   ],
   "source": [
    "# Fit the classifier to the training set\n",
    "xgb_clf.fit(X_train__preprocessed, y_train, \n",
    "            eval_set=[(X_test__preprocessed, y_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the labels of the test set: preds\n",
    "train_preds = xgb_clf.predict_proba(X_train__preprocessed)[:,1]\n",
    "test_preds = xgb_clf.predict_proba(X_test__preprocessed)[:,1]\n",
    "\n",
    "train_score = amex_score(y_train.values, train_preds)\n",
    "test_score = amex_score(y_test.values, test_preds)\n",
    "\n",
    "print(f'Train Score: {train_score}')\n",
    "print(f'Test Score: {test_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/feature_preprocessor.pkl','wb') as f:\n",
    "    pickle.dump(feature_preprocessor, f)\n",
    "\n",
    "with open('models/xgb_clf2.pkl','wb') as f:\n",
    "    pickle.dump(xgb_clf, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('amex_v2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "790edd3242a40ac70c58114a122b744731b030acc406feafcec626ff57999bc9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
