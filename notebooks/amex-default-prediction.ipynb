{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# American Express - Default Prediction\n",
    "Whether out at a restaurant or buying tickets to a concert, modern life counts on the convenience of a credit card to make daily purchases. It saves us from carrying large amounts of cash and also can advance a full purchase that can be paid over time. How do card issuers know we’ll pay back what we charge? That’s a complex problem with many existing solutions—and even more potential improvements, to be explored in this competition.\n",
    "## Introduction\n",
    "Credit default prediction is central to managing risk in a consumer lending business. Credit default prediction allows lenders to optimize lending decisions, which leads to a better customer experience and sound business economics. Current models exist to help manage risk. But it's possible to create better models that can outperform those currently in use.\n",
    "\n",
    "In this competition, we’ll apply supervised machine learning to predict credit default. Specifically, we will leverage an industrial scale dataset to build binary classifaction models that challenge the current model in production. Training, validation, and testing datasets include: time-series, behavioral data, and anonymized customer profile information. Apart from creating a base model, we will explore numerous techniques and methodolgies to create an impressive model through feature engineering and using the data in a more organic way within a model.\n",
    "### Objective\n",
    "The objective of this competition is to predict the probability that a customer does not pay back their credit card balance amount in the future based on their monthly customer profile. The target binary variable is calculated by observing 18 months performance window after the latest credit card statement, and if the customer does not pay due amount in 120 days after their latest statement date it is considered a default event.\n",
    "### Evaluation Criteria\n",
    "If successful, our solution when implemented may yield better customer experiences for cardholders by making it easier for them to be approved for a new credit card. Top solutions may even challenge the credit default prediction model used by the world's largest payment card issuer at American Express."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "### Project Setup and Configuration\n",
    "#### Notebook Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change working directory to project root\n",
    "import os\n",
    "\n",
    "if os.getcwd().split(\"/\")[-1] == \"notebooks\":\n",
    "    os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Description\n",
    "#### Files\n",
    "| Filename | Description |\n",
    "|-----------------|-----------------------|\n",
    "|`train_data.csv`   | training data with multiple statement dates per `customer_ID`|\n",
    "|`train_labels.csv` | target label for each `customer_ID`|\n",
    "|`test_data.csv`    | corresponding test data; goal: predict `target label` for each `customer_ID`|\n",
    "|`sample_submission.csv` | sample submission file in the correct format|\n",
    "#### Feature/Target Variables\n",
    "The dataset contains aggregated profile features for each customer at each statement date. \n",
    "\n",
    "Features are anonymized and normalized, and fall into the following general categories:\n",
    "| Prefix | Feature Type |\n",
    "|:------:|--------------|\n",
    "|`D_*`| Delinquency |\n",
    "|`S_*`| Spend |\n",
    "|`P_*`| Payment |\n",
    "|`B_*`| Balance |\n",
    "|`R_*`| Risk |\n",
    "\n",
    "with the following features being categorical: \n",
    "    `B_30`, `B_38`, `D_114`, `D_116`, `D_117`, `D_120`, `D_126`, `D_63`, `D_64`, `D_66`, `D_68`\n",
    "\n",
    "\n",
    "**Objective:** For each `customer_ID`, predict the probability of a future payment default (`target == 1`).\n",
    "\n",
    "**Note:** The negative class (`target == 0`) has been *subsampled at 5%*, and thus *receives a 20x weighting in the scoring metric*.\n",
    "\n",
    "**Data Source (AMEX):**\n",
    "\n",
    "American Express is a globally integrated payments company. As the largest payment card issuer in the world, they provide customers with access to products, insights, and experiences that enrich lives and build business success."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load AMEX Datasets\n",
    "**Source (Raw Data):**\n",
    "- https://www.kaggle.com/competitions/amex-default-prediction/data\n",
    "\n",
    "**Source (Compressed Data):**\n",
    "- https://www.kaggle.com/datasets/munumbutt/amexfeather\n",
    "\n",
    "// TODO: Add information regarding the source, advantages and limitations in using the compressed datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load compressed datasets\n",
    "# Source: https://www.kaggle.com/datasets/munumbutt/amexfeather\n",
    "\n",
    "train = pd.read_feather(\"./data/raw/train_data.ftr\")\n",
    "test = pd.read_feather(\"./data/raw/test_data.ftr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_ID</th>\n",
       "      <th>S_2</th>\n",
       "      <th>P_2</th>\n",
       "      <th>D_39</th>\n",
       "      <th>B_1</th>\n",
       "      <th>B_2</th>\n",
       "      <th>R_1</th>\n",
       "      <th>S_3</th>\n",
       "      <th>D_41</th>\n",
       "      <th>B_3</th>\n",
       "      <th>...</th>\n",
       "      <th>D_137</th>\n",
       "      <th>D_138</th>\n",
       "      <th>D_139</th>\n",
       "      <th>D_140</th>\n",
       "      <th>D_141</th>\n",
       "      <th>D_142</th>\n",
       "      <th>D_143</th>\n",
       "      <th>D_144</th>\n",
       "      <th>D_145</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...</td>\n",
       "      <td>2017-03-09</td>\n",
       "      <td>0.938477</td>\n",
       "      <td>0.001734</td>\n",
       "      <td>0.008728</td>\n",
       "      <td>1.006836</td>\n",
       "      <td>0.009224</td>\n",
       "      <td>0.124023</td>\n",
       "      <td>0.008774</td>\n",
       "      <td>0.004707</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002426</td>\n",
       "      <td>0.003706</td>\n",
       "      <td>0.003819</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000569</td>\n",
       "      <td>0.000610</td>\n",
       "      <td>0.002674</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...</td>\n",
       "      <td>2017-04-07</td>\n",
       "      <td>0.936523</td>\n",
       "      <td>0.005775</td>\n",
       "      <td>0.004925</td>\n",
       "      <td>1.000977</td>\n",
       "      <td>0.006153</td>\n",
       "      <td>0.126709</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>0.002714</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003956</td>\n",
       "      <td>0.003166</td>\n",
       "      <td>0.005032</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009575</td>\n",
       "      <td>0.005493</td>\n",
       "      <td>0.009216</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...</td>\n",
       "      <td>2017-05-28</td>\n",
       "      <td>0.954102</td>\n",
       "      <td>0.091492</td>\n",
       "      <td>0.021652</td>\n",
       "      <td>1.009766</td>\n",
       "      <td>0.006817</td>\n",
       "      <td>0.123962</td>\n",
       "      <td>0.007599</td>\n",
       "      <td>0.009422</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003269</td>\n",
       "      <td>0.007328</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003429</td>\n",
       "      <td>0.006985</td>\n",
       "      <td>0.002604</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...</td>\n",
       "      <td>2017-06-13</td>\n",
       "      <td>0.960449</td>\n",
       "      <td>0.002455</td>\n",
       "      <td>0.013687</td>\n",
       "      <td>1.002930</td>\n",
       "      <td>0.001372</td>\n",
       "      <td>0.117188</td>\n",
       "      <td>0.000685</td>\n",
       "      <td>0.005531</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006119</td>\n",
       "      <td>0.004517</td>\n",
       "      <td>0.003201</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008423</td>\n",
       "      <td>0.006527</td>\n",
       "      <td>0.009598</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...</td>\n",
       "      <td>2017-07-16</td>\n",
       "      <td>0.947266</td>\n",
       "      <td>0.002483</td>\n",
       "      <td>0.015190</td>\n",
       "      <td>1.000977</td>\n",
       "      <td>0.007607</td>\n",
       "      <td>0.117310</td>\n",
       "      <td>0.004654</td>\n",
       "      <td>0.009308</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003672</td>\n",
       "      <td>0.004944</td>\n",
       "      <td>0.008888</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001670</td>\n",
       "      <td>0.008125</td>\n",
       "      <td>0.009827</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 191 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         customer_ID        S_2       P_2  \\\n",
       "0  0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f... 2017-03-09  0.938477   \n",
       "1  0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f... 2017-04-07  0.936523   \n",
       "2  0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f... 2017-05-28  0.954102   \n",
       "3  0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f... 2017-06-13  0.960449   \n",
       "4  0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f... 2017-07-16  0.947266   \n",
       "\n",
       "       D_39       B_1       B_2       R_1       S_3      D_41       B_3  ...  \\\n",
       "0  0.001734  0.008728  1.006836  0.009224  0.124023  0.008774  0.004707  ...   \n",
       "1  0.005775  0.004925  1.000977  0.006153  0.126709  0.000798  0.002714  ...   \n",
       "2  0.091492  0.021652  1.009766  0.006817  0.123962  0.007599  0.009422  ...   \n",
       "3  0.002455  0.013687  1.002930  0.001372  0.117188  0.000685  0.005531  ...   \n",
       "4  0.002483  0.015190  1.000977  0.007607  0.117310  0.004654  0.009308  ...   \n",
       "\n",
       "   D_137  D_138     D_139     D_140     D_141  D_142     D_143     D_144  \\\n",
       "0    NaN    NaN  0.002426  0.003706  0.003819    NaN  0.000569  0.000610   \n",
       "1    NaN    NaN  0.003956  0.003166  0.005032    NaN  0.009575  0.005493   \n",
       "2    NaN    NaN  0.003269  0.007328  0.000427    NaN  0.003429  0.006985   \n",
       "3    NaN    NaN  0.006119  0.004517  0.003201    NaN  0.008423  0.006527   \n",
       "4    NaN    NaN  0.003672  0.004944  0.008888    NaN  0.001670  0.008125   \n",
       "\n",
       "      D_145  target  \n",
       "0  0.002674       0  \n",
       "1  0.009216       0  \n",
       "2  0.002604       0  \n",
       "3  0.009598       0  \n",
       "4  0.009827       0  \n",
       "\n",
       "[5 rows x 191 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview the first five rows of the training data\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data: Shape == (5531451, 191)\n",
      "\n",
      "The training set consists of 5531451 observations with 191 features and 1 target variable.\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of the DataFrame for the training set\n",
    "print(f\"Training Data: Shape == {train.shape}\")\n",
    "print(\n",
    "    f\"\\nThe training set consists of {train.shape[0]} observations with {train.shape[1]} features and 1 target variable.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5531451 entries, 0 to 5531450\n",
      "Columns: 191 entries, customer_ID to target\n",
      "dtypes: category(11), datetime64[ns](1), float16(177), int64(1), object(1)\n",
      "memory usage: 2.0+ GB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the first five rows of the testing set\n",
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Data: Shape == (11363762, 190)\n",
      "\n",
      "The testing set consists of 11363762 observations with 190 features.\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of the DataFrame for the testing dataset\n",
    "print(f\"Testing Data: Shape == {test.shape}\")\n",
    "print(\n",
    "    f\"\\nThe testing set consists of {test.shape[0]} observations with {test.shape[1]} features.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11363762 entries, 0 to 11363761\n",
      "Columns: 190 entries, customer_ID to D_145\n",
      "dtypes: category(11), datetime64[ns](1), float16(177), object(1)\n",
      "memory usage: 4.0+ GB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f34 features have 20% or greater have missing or null values\n",
      "{'B_17', 'D_43', 'D_88', 'D_110', 'D_105', 'D_53', 'D_135', 'R_9', 'B_39', 'D_87', 'D_82', 'D_138', 'B_42', 'S_9', 'D_76', 'D_73', 'D_108', 'D_136', 'D_132', 'D_142', 'D_77', 'D_46', 'S_27', 'D_42', 'R_26', 'D_137', 'B_29', 'D_66', 'D_111', 'D_50', 'D_49', 'D_56', 'D_106', 'D_134'}\n"
     ]
    }
   ],
   "source": [
    "# 1-2) Sum the number of incomplete (missing or null) values in each column\n",
    "# 3-4) Divide by the number of observations and multipy by 100 to make it a percentage.\n",
    "#   5) Lastly, sort the values in descending order to better observe feature incompleteness.\n",
    "pct_incomplete = (\n",
    "    train.isna().sum().div(len(train)).mul(100).sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "# Subset pct_incomplete to select incomplete features (Threshold: >20%)\n",
    "incomplete_features = set(pct_incomplete[pct_incomplete >= 20].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 features with over 20% values are missing or null.\n",
      "\n",
      "Incomplete Features: \n",
      "{'B_17', 'D_43', 'D_88', 'D_110', 'D_105', 'D_53', 'D_135', 'R_9', 'B_39', 'D_87', 'D_82', 'D_138', 'B_42', 'S_9', 'D_76', 'D_73', 'D_108', 'D_136', 'D_132', 'D_142', 'D_77', 'D_46', 'S_27', 'D_42', 'R_26', 'D_137', 'B_29', 'D_66', 'D_111', 'D_50', 'D_49', 'D_56', 'D_106', 'D_134'}\n"
     ]
    }
   ],
   "source": [
    "# Print the count of incomplete features\n",
    "print(\n",
    "    f\"{len(incomplete_features)} features with over 20% values are missing or null.\\n\"\n",
    ")\n",
    "\n",
    "# Print column names of features where 20% or greater have missing or null values\n",
    "print(f\"Incomplete Features: \\n{incomplete_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "6253b0757717560d5dcadf36acf1738a8f43823bd2758e22944cdc4c86c7691f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
